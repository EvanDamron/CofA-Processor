# -*- coding: utf-8 -*-
"""NLP_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17la3s3EdHXVm0NjXrH5GzW6CgrMEKxv-
"""

pip install transformers datasets torch

from datasets import Dataset

# Sample data with text and corresponding NER labels
data = {
    'text': [
        "Chevron Oronite Co LLC shipped OLOA® 55603 with batch number 2025021820.",
        "The product OLOA® 55603 was shipped with quantity 21,078.652 gallons.",
        "The batch number for the shipment is 2025021820, which was produced on 01/12/2025."
    ],
    'labels': [
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],  # Entity labels for first sentence (Chevron, Product Name, Batch Number)
        [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],     # Entity labels for second sentence (Product, Quantity)
        [0, 0, 0, 1, 0, 0, 0, 0, 1, 0]      # Entity labels for third sentence (Batch Number, Production Date)
    ]
}

# Create a Hugging Face dataset
dataset = Dataset.from_dict(data)

# Display the dataset to verify its contents
print(dataset)

def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# Tokenize the entire dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Display the tokenized dataset
print(tokenized_dataset)

from transformers import DataCollatorForTokenClassification

# Create a data collator
data_collator = DataCollatorForTokenClassification(tokenizer)

from transformers import TrainingArguments

# Set up training arguments
training_args = TrainingArguments(
    output_dir='./results',           # output directory
    evaluation_strategy="epoch",      # evaluate after each epoch
    learning_rate=2e-5,               # learning rate
    per_device_train_batch_size=16,   # batch size for training
    per_device_eval_batch_size=64,    # batch size for evaluation
    num_train_epochs=3,               # number of training epochs
    weight_decay=0.01,                # weight decay
    logging_dir='./logs',             # logging directory
    report_to="none",                 # disable WandB logging
)

from transformers import Trainer

trainer = Trainer(
    model=model,                         # The model to train
    args=training_args,                  # Training arguments
    train_dataset=tokenized_dataset,     # Training dataset
    eval_dataset=tokenized_dataset,      # Evaluation dataset (same as training for now)
    data_collator=data_collator,         # Data collator for batching
)

# Train the model
trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()

# Print evaluation results
print("Evaluation results:", eval_results)

# Check the model's label map
print(model.config.id2label)

import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load pre-trained tokenizer and model for NER
model_name = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Example label map for NER tasks
label_map = {
    0: 'O',        # Outside any named entity
    1: 'B-MISC',   # Beginning of a Miscellaneous entity
    2: 'I-MISC',   # Inside a Miscellaneous entity
    3: 'B-PER',    # Beginning of a Person entity
    4: 'I-PER',    # Inside a Person entity
    5: 'B-ORG',    # Beginning of an Organization entity
    6: 'I-ORG',    # Inside an Organization entity
    7: 'B-LOC',    # Beginning of a Location entity
    8: 'I-LOC'     # Inside a Location entity
}
# Step 1: Read the testing file
test_file_path = Path("/content/Raw Material_Evan.txt")  # Replace with your actual file path
test_text = test_file_path.read_text()

# Step 2: Tokenize the test text
def tokenize_test_data(text):
    return tokenizer(text, padding="max_length", truncation=True, return_tensors="pt")

tokenized_test_data = tokenize_test_data(test_text)

# Step 3: Predict the entities
with torch.no_grad():
    outputs = model(**tokenized_test_data)

# Step 4: Convert logits to predicted labels
logits = outputs.logits
predicted_labels = torch.argmax(logits, dim=-1)

# Convert the predicted labels back to entity labels
predicted_entities = predicted_labels[0].tolist()

# Step 5: Map predicted labels to entity names
tokens = tokenizer.convert_ids_to_tokens(tokenized_test_data["input_ids"][0])
predicted_entities_named = [label_map[label] for label in predicted_entities]

# Step 6: Display the results
tokens = tokenizer.convert_ids_to_tokens(tokenized_test_data["input_ids"][0])
for token, entity in zip(tokens, predicted_entities_named):
    print(f"{token}: {entity}")

